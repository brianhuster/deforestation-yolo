{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n\n# How to Train YOLOv8 Object Detection on a Custom Dataset\n\n---\n\n[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset)\n[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/wuZtUMEiKWY)\n[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ultralytics/ultralytics)\n\nUltralytics YOLOv8 is the latest version of the YOLO (You Only Look Once) object detection and image segmentation model developed by Ultralytics. The YOLOv8 model is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and image segmentation tasks. It can be trained on large datasets and is capable of running on a variety of hardware platforms, from CPUs to GPUs.\n\n## ‚ö†Ô∏è Disclaimer\n\nYOLOv8 is still under heavy development. Breaking changes are being introduced almost weekly. We strive to make our YOLOv8 notebooks work with the latest version of the library. Last tests took place on **03.01.2024** with version **YOLOv8.0.196**.\n\nIf you notice that our notebook behaves incorrectly - especially if you experience errors that prevent you from going through the tutorial - don't hesitate! Let us know and open an [issue](https://github.com/roboflow/notebooks/issues) on the Roboflow Notebooks repository.\n\n## Accompanying Blog Post\n\nWe recommend that you follow along in this notebook while reading the blog post on how to train YOLOv8 Object Detection, concurrently.\n\n## Pro Tip: Use GPU Acceleration\n\nIf you are running this notebook in Google Colab, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`. This will ensure your notebook uses a GPU, which will significantly speed up model training times.\n\n## Steps in this Tutorial\n\nIn this tutorial, we are going to cover:\n\n- Before you start\n- Install YOLOv8\n- CLI Basics\n- Inference with Pre-trained COCO Model\n- Roboflow Universe\n- Preparing a custom dataset\n- Custom Training\n- Validate Custom Model\n- Inference with Custom Model\n\n**Let's begin!**","metadata":{"id":"oe9vkEvFABbN"}},{"cell_type":"markdown","source":"## Before you start\n\nLet's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`.","metadata":{"id":"FyRdDYkqAKN4"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y8cDtxLIBHgQ","outputId":"fd9f49a5-1c29-441d-c3e2-b6e452602c26","execution":{"iopub.status.busy":"2024-09-09T23:19:36.700070Z","iopub.execute_input":"2024-09-09T23:19:36.700690Z","iopub.status.idle":"2024-09-09T23:19:37.742963Z","shell.execute_reply.started":"2024-09-09T23:19:36.700640Z","shell.execute_reply":"2024-09-09T23:19:37.741998Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Mon Sep  9 23:19:37 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   37C    P0             26W /  250W |       0MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nHOME = os.getcwd()\nprint(HOME)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CjpPg4mGKc1v","outputId":"0cc05171-8428-4281-9177-14a0aa53a2dc","execution":{"iopub.status.busy":"2024-09-09T23:19:41.820886Z","iopub.execute_input":"2024-09-09T23:19:41.821260Z","iopub.status.idle":"2024-09-09T23:19:41.826892Z","shell.execute_reply.started":"2024-09-09T23:19:41.821226Z","shell.execute_reply":"2024-09-09T23:19:41.825977Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"markdown","source":"git push --set-upstream origin master## Install YOLOv8\n\n‚ö†Ô∏è YOLOv8 is still under heavy development. Breaking changes are being introduced almost weekly. We strive to make our YOLOv8 notebooks work with the latest version of the library. Last tests took place on **03.01.2024** with version **YOLOv8.0.196**.\n\nIf you notice that our notebook behaves incorrectly - especially if you experience errors that prevent you from going through the tutorial - don't hesitate! Let us know and open an [issue](https://github.com/roboflow/notebooks/issues) on the Roboflow Notebooks repository.\n\nYOLOv8 can be installed in two ways‚Ää-‚Ääfrom the source and via pip. This is because it is the first iteration of YOLO to have an official package.","metadata":{"id":"3C3EO_2zNChu"}},{"cell_type":"code","source":"# Pip install method (recommended)\n\n!pip install ultralytics==8.0.196\n\nfrom IPython import display\ndisplay.clear_output()\n\nimport ultralytics\nultralytics.checks()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tdSMcABDNKW-","outputId":"dbea2a9e-4563-45d7-fffa-1028e51096b2","execution":{"iopub.status.busy":"2024-09-09T23:37:12.555616Z","iopub.execute_input":"2024-09-09T23:37:12.556582Z","iopub.status.idle":"2024-09-09T23:37:25.552833Z","shell.execute_reply.started":"2024-09-09T23:37:12.556539Z","shell.execute_reply":"2024-09-09T23:37:25.551820Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Ultralytics YOLOv8.0.196 üöÄ Python-3.10.14 torch-2.4.0 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nSetup complete ‚úÖ (4 CPUs, 31.4 GB RAM, 5846.0/8062.4 GB disk)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install ray[tune]==2.9.3","metadata":{"execution":{"iopub.status.busy":"2024-09-09T23:38:32.920763Z","iopub.execute_input":"2024-09-09T23:38:32.921163Z","iopub.status.idle":"2024-09-09T23:38:54.562784Z","shell.execute_reply.started":"2024-09-09T23:38:32.921128Z","shell.execute_reply":"2024-09-09T23:38:54.561627Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Collecting ray==2.9.3 (from ray[tune]==2.9.3)\n  Downloading ray-2.9.3-cp310-cp310-manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from ray==2.9.3->ray[tune]==2.9.3) (8.1.7)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from ray==2.9.3->ray[tune]==2.9.3) (3.15.1)\nRequirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from ray==2.9.3->ray[tune]==2.9.3) (4.22.0)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray==2.9.3->ray[tune]==2.9.3) (1.0.8)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ray==2.9.3->ray[tune]==2.9.3) (21.3)\nRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.10/site-packages (from ray==2.9.3->ray[tune]==2.9.3) (3.20.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from ray==2.9.3->ray[tune]==2.9.3) (6.0.2)\nRequirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray==2.9.3->ray[tune]==2.9.3) (1.3.1)\nRequirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray==2.9.3->ray[tune]==2.9.3) (1.4.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from ray==2.9.3->ray[tune]==2.9.3) (2.32.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from ray[tune]==2.9.3) (2.2.2)\nRequirement already satisfied: tensorboardX>=1.9 in /opt/conda/lib/python3.10/site-packages (from ray[tune]==2.9.3) (2.6.2.2)\nRequirement already satisfied: pyarrow>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from ray[tune]==2.9.3) (16.1.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from ray[tune]==2.9.3) (2024.6.1)\nRequirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.10/site-packages (from pyarrow>=6.0.1->ray[tune]==2.9.3) (1.26.4)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.9.3->ray[tune]==2.9.3) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.9.3->ray[tune]==2.9.3) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.9.3->ray[tune]==2.9.3) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.9.3->ray[tune]==2.9.3) (0.18.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->ray==2.9.3->ray[tune]==2.9.3) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->ray[tune]==2.9.3) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->ray[tune]==2.9.3) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->ray[tune]==2.9.3) (2024.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.9.3->ray[tune]==2.9.3) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.9.3->ray[tune]==2.9.3) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.9.3->ray[tune]==2.9.3) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.9.3->ray[tune]==2.9.3) (2024.7.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->ray[tune]==2.9.3) (1.16.0)\nDownloading ray-2.9.3-cp310-cp310-manylinux2014_x86_64.whl (64.9 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.9/64.9 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: ray\n  Attempting uninstall: ray\n    Found existing installation: ray 2.24.0\n    Uninstalling ray-2.24.0:\n      Successfully uninstalled ray-2.24.0\nSuccessfully installed ray-2.9.3\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/brianhuster/anti-deforestation-dataset-for-yolo","metadata":{"id":"iVvaIYEEPOty","execution":{"iopub.status.busy":"2024-09-09T23:21:48.214990Z","iopub.execute_input":"2024-09-09T23:21:48.215901Z","iopub.status.idle":"2024-09-09T23:22:00.807590Z","shell.execute_reply.started":"2024-09-09T23:21:48.215860Z","shell.execute_reply":"2024-09-09T23:22:00.806673Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Cloning into 'anti-deforestation-dataset-for-yolo'...\nremote: Enumerating objects: 5915, done.\u001b[K\nremote: Total 5915 (delta 0), reused 0 (delta 0), pack-reused 5915 (from 1)\u001b[K\nReceiving objects: 100% (5915/5915), 380.46 MiB | 49.98 MiB/s, done.\nResolving deltas: 100% (3/3), done.\nUpdating files: 100% (7657/7657), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"from ultralytics import YOLO\n\nfrom IPython.display import display, Image","metadata":{"id":"VOEYrlBoP9-E","execution":{"iopub.status.busy":"2024-09-09T23:22:44.182981Z","iopub.execute_input":"2024-09-09T23:22:44.183368Z","iopub.status.idle":"2024-09-09T23:22:44.188177Z","shell.execute_reply.started":"2024-09-09T23:22:44.183333Z","shell.execute_reply":"2024-09-09T23:22:44.187254Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Custom Training","metadata":{"id":"YUjFBKKqXa-u"}},{"cell_type":"code","source":"%cd {HOME}\n\n!yolo task=detect mode=train model=yolov8s.pt data=/kaggle/working/anti-deforestation-dataset-for-yolo/data.yaml epochs=25 imgsz=800 plots=True","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D2YkphuiaE7_","outputId":"a7414341-df8a-4ef6-e772-383827b481fe","execution":{"iopub.status.busy":"2024-09-09T23:26:33.840528Z","iopub.execute_input":"2024-09-09T23:26:33.840970Z","iopub.status.idle":"2024-09-09T23:28:47.653531Z","shell.execute_reply.started":"2024-09-09T23:26:33.840930Z","shell.execute_reply":"2024-09-09T23:28:47.652535Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/kaggle/working\nDownloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt to 'yolov8s.pt'...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21.5M/21.5M [00:00<00:00, 229MB/s]\n/opt/conda/lib/python3.10/site-packages/ultralytics/nn/tasks.py:567: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(file, map_location='cpu'), file  # load\nNew https://pypi.org/project/ultralytics/8.2.91 available üòÉ Update with 'pip install -U ultralytics'\nUltralytics YOLOv8.0.196 üöÄ Python-3.10.14 torch-2.4.0 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/kaggle/working/anti-deforestation-dataset-for-yolo/data.yaml, epochs=25, patience=50, batch=16, imgsz=800, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:00<00:00, 20.1MB/s]\nOverriding model.yaml nc=80 with nc=3\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n 22        [15, 18, 21]  1   2117209  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \nModel summary: 225 layers, 11136761 parameters, 11136745 gradients, 28.7 GFLOPs\n\nTransferred 349/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\nDownloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.23M/6.23M [00:00<00:00, 115MB/s]\n/opt/conda/lib/python3.10/site-packages/ultralytics/nn/tasks.py:567: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(file, map_location='cpu'), file  # load\n/opt/conda/lib/python3.10/site-packages/ultralytics/utils/checks.py:558: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(True):\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:238: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = amp.GradScaler(enabled=self.amp)\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/anti-deforestation-dataset-for-yolo/labels/train\u001b[0m\n\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/anti-deforestation-dataset-for-yolo/labels/train.cache\n/opt/conda/lib/python3.10/site-packages/albumentations/core/composition.py:161: UserWarning: Got processor for bboxes, but no transform to process it.\n  self._set_keys()\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/anti-deforestation-dataset-for-yolo/labels/val... \u001b[0m\n\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/anti-deforestation-dataset-for-yolo/labels/val.cache\nPlotting labels to runs/detect/train/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\nImage sizes 800 train, 800 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mruns/detect/train\u001b[0m\nStarting training for 25 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/25      5.93G       1.87      3.599      1.957          7        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        383        290       0.38      0.395      0.222      0.103\nTraceback (most recent call last):\n  File \"/opt/conda/bin/yolo\", line 8, in <module>\n    sys.exit(entrypoint())\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/cfg/__init__.py\", line 445, in entrypoint\n    getattr(model, mode)(**overrides)  # default args from model\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/engine/model.py\", line 341, in train\n    self.trainer.train()\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py\", line 191, in train\n    self._do_train(world_size)\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py\", line 396, in _do_train\n    self.run_callbacks('on_fit_epoch_end')\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py\", line 156, in run_callbacks\n    callback(self)\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/utils/callbacks/raytune.py\", line 17, in on_fit_epoch_end\n    if ray.tune.is_session_enabled():\nAttributeError: module 'ray.tune' has no attribute 'is_session_enabled'\nSentry is attempting to send 2 pending events\nWaiting up to 2 seconds\nPress Ctrl-C to quit\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls {HOME}/runs/detect/train/","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1MScstfHhArr","outputId":"210f2b1e-aea6-464b-d69e-319c473338a4","execution":{"iopub.status.busy":"2024-09-09T02:51:28.289119Z","iopub.status.idle":"2024-09-09T02:51:28.289727Z","shell.execute_reply.started":"2024-09-09T02:51:28.289459Z","shell.execute_reply":"2024-09-09T02:51:28.289486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd {HOME}\nImage(filename=f'{HOME}/runs/detect/train/confusion_matrix.png', width=600)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":484},"id":"_J35i8Ofhjxa","outputId":"3584e96f-5a55-4391-c51f-3acf53f80cd9","execution":{"iopub.status.busy":"2024-09-09T02:51:28.291279Z","iopub.status.idle":"2024-09-09T02:51:28.291800Z","shell.execute_reply.started":"2024-09-09T02:51:28.291538Z","shell.execute_reply":"2024-09-09T02:51:28.291565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd {HOME}\nImage(filename=f'{HOME}/runs/detect/train/results.png', width=600)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":334},"id":"A-urTWUkhRmn","outputId":"836e9053-7035-48ba-ef10-9f7155a329de","execution":{"iopub.status.busy":"2024-09-09T02:51:28.293240Z","iopub.status.idle":"2024-09-09T02:51:28.293752Z","shell.execute_reply.started":"2024-09-09T02:51:28.293490Z","shell.execute_reply":"2024-09-09T02:51:28.293514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd {HOME}\nImage(filename=f'{HOME}/runs/detect/train/val_batch0_pred.jpg', width=600)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":381},"id":"HI4nADCCj3F5","outputId":"ad7f1e75-222c-4097-ee6a-edcff68ff723","execution":{"iopub.status.busy":"2024-09-09T02:51:28.295310Z","iopub.status.idle":"2024-09-09T02:51:28.295801Z","shell.execute_reply.started":"2024-09-09T02:51:28.295549Z","shell.execute_reply":"2024-09-09T02:51:28.295573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validate Custom Model","metadata":{"id":"6ODk1VTlevxn"}},{"cell_type":"code","source":"%cd {HOME}\n\n!yolo task=detect mode=val model={HOME}/runs/detect/train/weights/best.pt data={dataset.location}/data.yaml","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YpyuwrNlXc1P","outputId":"f0bb32ee-5da7-4249-f6e8-bb19ef860b4d","execution":{"iopub.status.busy":"2024-09-09T02:51:28.297041Z","iopub.status.idle":"2024-09-09T02:51:28.297551Z","shell.execute_reply.started":"2024-09-09T02:51:28.297288Z","shell.execute_reply":"2024-09-09T02:51:28.297315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference with Custom Model","metadata":{"id":"i4eASbcWkQBq"}},{"cell_type":"code","source":"%cd {HOME}\n!yolo task=detect mode=predict model={HOME}/runs/detect/train/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wjc1ctZykYuf","outputId":"95967de2-7d16-414a-dfc5-7c8eb38065e6","execution":{"iopub.status.busy":"2024-09-09T02:51:28.299520Z","iopub.status.idle":"2024-09-09T02:51:28.299997Z","shell.execute_reply.started":"2024-09-09T02:51:28.299744Z","shell.execute_reply":"2024-09-09T02:51:28.299768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**NOTE:** Let's take a look at few results.","metadata":{"id":"mEYIo95n-I0S"}},{"cell_type":"code","source":"import glob\nfrom IPython.display import Image, display\n\n# Define the base path where the folders are located\nbase_path = '/content/runs/detect/'\n\n# List all directories that start with 'predict' in the base path\nsubfolders = [os.path.join(base_path, d) for d in os.listdir(base_path)\n              if os.path.isdir(os.path.join(base_path, d)) and d.startswith('predict')]\n\n# Find the latest folder by modification time\nlatest_folder = max(subfolders, key=os.path.getmtime)\n\nimage_paths = glob.glob(f'{latest_folder}/*.jpg')[:3]\n\n# Display each image\nfor image_path in image_paths:\n    display(Image(filename=image_path, width=600))\n    print(\"\\n\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"jbVjEtPAkz3j","outputId":"94a5d40d-62e8-4347-bd4b-183aae9e002a","execution":{"iopub.status.busy":"2024-09-09T02:51:28.301272Z","iopub.status.idle":"2024-09-09T02:51:28.301619Z","shell.execute_reply.started":"2024-09-09T02:51:28.301449Z","shell.execute_reply":"2024-09-09T02:51:28.301466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deploy model on Roboflow\n\nOnce you have finished training your YOLOv8 model, you‚Äôll have a set of trained weights ready for use. These weights will be in the `/runs/detect/train/weights/best.pt` folder of your project. You can upload your model weights to Roboflow Deploy to use your trained weights on our infinitely scalable infrastructure.\n\nThe `.deploy()` function in the [Roboflow pip package](https://docs.roboflow.com/python) now supports uploading YOLOv8 weights.\n\nTo upload model weights, add the following code to the ‚ÄúInference with Custom Model‚Äù section in the aforementioned notebook:","metadata":{"id":"j0tsVilOCPyq"}},{"cell_type":"code","source":"project.version(dataset.version).deploy(model_type=\"yolov8\", model_path=f\"{HOME}/runs/detect/train/\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6EhBAJ2gCPZh","outputId":"259decf5-1c4e-4011-a208-a2498acc30ca","execution":{"iopub.status.busy":"2024-09-09T02:51:28.303225Z","iopub.status.idle":"2024-09-09T02:51:28.303573Z","shell.execute_reply.started":"2024-09-09T02:51:28.303403Z","shell.execute_reply":"2024-09-09T02:51:28.303420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#While your deployment is processing, checkout the deployment docs to take your model to most destinations https://docs.roboflow.com/inference","metadata":{"id":"q5kOhjkmcV1l","execution":{"iopub.status.busy":"2024-09-09T02:51:28.304810Z","iopub.status.idle":"2024-09-09T02:51:28.305202Z","shell.execute_reply.started":"2024-09-09T02:51:28.304985Z","shell.execute_reply":"2024-09-09T02:51:28.305003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Run inference on your model on a persistant, auto-scaling, cloud API\n\n#load model\nmodel = project.version(dataset.version).model\n\n#choose random test set image\nimport os, random\ntest_set_loc = dataset.location + \"/test/images/\"\nrandom_test_image = random.choice(os.listdir(test_set_loc))\nprint(\"running inference on \" + random_test_image)\n\npred = model.predict(test_set_loc + random_test_image, confidence=40, overlap=30).json()\npred","metadata":{"id":"I4bpUIibcV1l","execution":{"iopub.status.busy":"2024-09-09T02:51:28.306266Z","iopub.status.idle":"2024-09-09T02:51:28.306627Z","shell.execute_reply.started":"2024-09-09T02:51:28.306449Z","shell.execute_reply":"2024-09-09T02:51:28.306468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deploy Your Model to the Edge\n\nIn addition to using the Roboflow hosted API for deployment, you can use [Roboflow Inference](https://inference.roboflow.com), an open source inference solution that has powered millions of API calls in production environments. Inference works with CPU and GPU, giving you immediate access to a range of devices, from the NVIDIA Jetson to TRT-compatible devices to ARM CPU devices.\n\nWith Roboflow Inference, you can self-host and deploy your model on-device. You can deploy applications using the [Inference Docker containers](https://inference.roboflow.com/quickstart/docker/) or the pip package.\n\nFor example, to install Inference on a device with an NVIDIA GPU, we can use:\n\n```\ndocker pull roboflow/roboflow-inference-server-gpu\n```\n\nThen we can run inference via HTTP:\n\n```python\nimport requests\n\nworkspace_id = \"\"\nmodel_id = \"\"\nimage_url = \"\"\nconfidence = 0.75\napi_key = \"\"\n\ninfer_payload = {\n    \"image\": {\n        \"type\": \"url\",\n        \"value\": image_url,\n    },\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\nres = requests.post(\n    f\"http://localhost:9001/{workspace_id}/{model_id}\",\n    json=infer_object_detection_payload,\n)\n\npredictions = res.json()\n```\n\nAbove, set your Roboflow workspace ID, model ID, and API key.\n\n- [Find your workspace and model ID](https://docs.roboflow.com/api-reference/workspace-and-project-ids?ref=blog.roboflow.com)\n- [Find your API key](https://docs.roboflow.com/api-reference/authentication?ref=blog.roboflow.com#retrieve-an-api-key)\n\nAlso, set the URL of an image on which you want to run inference. This can be a local file.\n\n_To use your YOLOv5 model commercially with Inference, you will need a Roboflow Enterprise license, through which you gain a pass-through license for using YOLOv5. An enterprise license also grants you access to features like advanced device management, multi-model containers, auto-batch inference, and more._","metadata":{"id":"wfsZwITuBQAW"}},{"cell_type":"code","source":"","metadata":{"id":"dg-pCCPcBQAW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## üèÜ Congratulations\n\n### Learning Resources\n\nRoboflow has produced many resources that you may find interesting as you advance your knowledge of computer vision:\n\n- [Roboflow Notebooks](https://github.com/roboflow/notebooks): A repository of over 20 notebooks that walk through how to train custom models with a range of model types, from YOLOv7 to SegFormer.\n- [Roboflow YouTube](https://www.youtube.com/c/Roboflow): Our library of videos featuring deep dives into the latest in computer vision, detailed tutorials that accompany our notebooks, and more.\n- [Roboflow Discuss](https://discuss.roboflow.com/): Have a question about how to do something on Roboflow? Ask your question on our discussion forum.\n- [Roboflow Models](https://roboflow.com): Learn about state-of-the-art models and their performance. Find links and tutorials to guide your learning.\n\n### Convert data formats\n\nRoboflow provides free utilities to convert data between dozens of popular computer vision formats. Check out [Roboflow Formats](https://roboflow.com/formats) to find tutorials on how to convert data between formats in a few clicks.\n\n### Connect computer vision to your project logic\n\n[Roboflow Templates](https://roboflow.com/templates) is a public gallery of code snippets that you can use to connect computer vision to your project logic. Code snippets range from sending emails after inference to measuring object distance between detections.","metadata":{"id":"ovQgOj_xSNDg"}}]}